{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training and test datasets...\n"
     ]
    }
   ],
   "source": [
    "print 'Loading the training and test datasets...'\n",
    "\n",
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n"
     ]
    }
   ],
   "source": [
    "print 'Loading training dataset'\n",
    "train_file = pd.read_csv(data_path + 'train.csv', dtype= {'msno' : 'category', 'source_system_tab':'category', 'source_screen_name':'category',\n",
    "                                                           'source_type' : 'category', 'target' : np.uint8, 'song_id' : 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           msno  \\\n",
      "0  FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=   \n",
      "1  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
      "2  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
      "3  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
      "4  FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=   \n",
      "\n",
      "                                        song_id source_system_tab  \\\n",
      "0  BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik=           explore   \n",
      "1  bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM=        my library   \n",
      "2  JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY=        my library   \n",
      "3  2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs=        my library   \n",
      "4  3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc=           explore   \n",
      "\n",
      "    source_screen_name      source_type  target  \n",
      "0              Explore  online-playlist       1  \n",
      "1  Local playlist more   local-playlist       1  \n",
      "2  Local playlist more   local-playlist       1  \n",
      "3  Local playlist more   local-playlist       1  \n",
      "4              Explore  online-playlist       1  \n"
     ]
    }
   ],
   "source": [
    "print pd.DataFrame.head(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset\n"
     ]
    }
   ],
   "source": [
    "print 'Loading test dataset'\n",
    "test = pd.read_csv(data_path + 'test.csv', dtype={'msno' : 'category',\n",
    "                                                'source_system_tab' : 'category',\n",
    "                                                'source_screen_name' : 'category',\n",
    "                                                'source_type' : 'category',\n",
    "                                                'song_id' : 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                          msno  \\\n",
      "0   0  V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM=   \n",
      "1   1  V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM=   \n",
      "2   2  /uQAlrAkaczV+nWCd2sPF2ekvXPRipV7q0l+gbLuxjw=   \n",
      "3   3  1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k=   \n",
      "4   4  1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k=   \n",
      "\n",
      "                                        song_id source_system_tab  \\\n",
      "0  WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc=        my library   \n",
      "1  y/rsZ9DC7FwK5F2PK2D5mj+aOBUJAjuu3dZ14NgE0vM=        my library   \n",
      "2  8eZLFOdGVdXBSqoAv5nsLigeH2BvKXzTQYtUM53I0k4=          discover   \n",
      "3  ztCf8thYsS4YN3GcIL/bvoxLm/T5mYBVKOO4C9NiVfQ=             radio   \n",
      "4  MKVMpslKcQhMaFEgcEQhEfi5+RZhMYlU3eRDpySrH8Y=             radio   \n",
      "\n",
      "    source_screen_name          source_type  \n",
      "0  Local playlist more        local-library  \n",
      "1  Local playlist more        local-library  \n",
      "2                  NaN  song-based-playlist  \n",
      "3                Radio                radio  \n",
      "4                Radio                radio  \n"
     ]
    }
   ],
   "source": [
    "print pd.DataFrame.head(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading song list\n"
     ]
    }
   ],
   "source": [
    "print 'Loading song list'\n",
    "\n",
    "songs = pd.read_csv(data_path + 'songs.csv',dtype={'genre_ids': 'category',\n",
    "                                                  'language' : 'category',\n",
    "                                                  'artist_name' : 'category',\n",
    "                                                  'composer' : 'category',\n",
    "                                                  'lyricist' : 'category',\n",
    "                                                  'song_id' : 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        song_id  song_length genre_ids  \\\n",
      "0  CXoTN1eb7AI+DntdU1vbcwGRV4SCIDxZu+YD8JP8r4E=       247640       465   \n",
      "1  o0kFgae9QtnYgRkVPqLJwa05zIhRlUjfF7O1tDw0ZDU=       197328       444   \n",
      "2  DwVvVurfpuz+XPuFvucclVQEyPqcpUkHR0ne1RQzPs0=       231781       465   \n",
      "3  dKMBWoZyScdxSkihKG+Vf47nc18N9q4m58+b4e7dSSE=       273554       465   \n",
      "4  W3bqWd3T+VeHFzHAUfARgW9AvVRaF4N5Yzm4Mr6Eo/o=       140329       726   \n",
      "\n",
      "        artist_name                            composer     lyricist language  \n",
      "0  張信哲 (Jeff Chang)                                  董貞          何啟弘      3.0  \n",
      "1         BLACKPINK  TEDDY|  FUTURE BOUNCE|  Bekuh BOOM        TEDDY     31.0  \n",
      "2      SUPER JUNIOR                                 NaN          NaN     31.0  \n",
      "3             S.H.E                                 湯小康          徐世珍      3.0  \n",
      "4              貴族精選                         Traditional  Traditional     52.0  \n"
     ]
    }
   ],
   "source": [
    "print pd.DataFrame.head(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading members list\n"
     ]
    }
   ],
   "source": [
    "print 'Loading members list'\n",
    "\n",
    "members = pd.read_csv(data_path + 'members.csv',dtype={'city' : 'category',\n",
    "                                                      'bd' : np.uint8,\n",
    "                                                      'gender' : 'category',\n",
    "                                                      'registered_via' : 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           msno city  bd gender  \\\n",
      "0  XQxgAYj3klVKjR3oxPPXYYFp4soD4TuBghkhMTD4oTw=    1   0    NaN   \n",
      "1  UizsfmJb9mV54qE9hCYyU07Va97c0lCRLEQX3ae+ztM=    1   0    NaN   \n",
      "2  D8nEhsIOBSoE6VthTaqDX8U6lqjJ7dLdr72mOyLya2A=    1   0    NaN   \n",
      "3  mCuD+tZ1hERA/o5GPqk38e041J8ZsBaLcu7nGoIIvhI=    1   0    NaN   \n",
      "4  q4HRBfVSssAFS9iRfxWrohxuk9kCYMKjHOEagUMV6rQ=    1   0    NaN   \n",
      "\n",
      "  registered_via  registration_init_time  expiration_date  \n",
      "0              7                20110820         20170920  \n",
      "1              7                20150628         20170622  \n",
      "2              4                20160411         20170712  \n",
      "3              9                20150906         20150907  \n",
      "4              4                20170126         20170613  \n"
     ]
    }
   ],
   "source": [
    "print pd.DataFrame.head(members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading songs extra info\n"
     ]
    }
   ],
   "source": [
    "print 'Loading songs extra info'\n",
    "\n",
    "songs_extra = pd.read_csv(data_path + 'song_extra_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        song_id             name          isrc\n",
      "0  LP7pLJoJFBvyuUwvu+oLzjT+bI+UeBPURCecJsX1jjs=               我們  TWUM71200043\n",
      "1  ClazTFnk6r0Bnuie44bocdNMM3rdlrq0bCGAsGUWcHE=  Let Me Love You  QMZSY1600015\n",
      "2  u2ja/bZE3zhCGxvbbOB3zOoUjx27u40cf5g09UXMoKQ=              原諒我  TWA530887303\n",
      "3  92Fqsy0+p6+RHe2EoLKjHahORHR1Kq1TBJoClW9v+Ts=          Classic  USSM11301446\n",
      "4  0QFmz/+rJy1Q56C1DuYqT9hKKqi5TUqx0sN0IwvoHrw=             愛投羅網  TWA471306001\n"
     ]
    }
   ],
   "source": [
    "print pd.DataFrame.head(songs_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing the data\n"
     ]
    }
   ],
   "source": [
    "print 'Pre-processing the data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a join on traing_file and songs, using the song_id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "song_cols = ['song_id', 'artist_name', 'genre_ids', 'song_length', 'language']\n",
    "train_file = train_file.merge(songs[song_cols], on='song_id', how='left')\n",
    "test = test.merge(songs[song_cols], on='song_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we change the registration and expiry time to DD/MM/YYYY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "members['registration_year'] = members['registration_init_time'].apply(lambda x: int(str(x)[0:4]))\n",
    "members['registration_month'] = members['registration_init_time'].apply(lambda x: int(str(x)[4:6]))\n",
    "members['registration_date'] = members['registration_init_time'].apply(lambda x: int(str(x)[6:8]))\n",
    "\n",
    "members['expiration_year'] = members['expiration_date'].apply(lambda x: int(str(x)[0:4]))\n",
    "members['expiration_month'] = members['expiration_date'].apply(lambda x: int(str(x)[4:6]))\n",
    "members['expiration_date'] = members['expiration_date'].apply(lambda x: int(str(x)[6:8]))\n",
    "members = members.drop(['registration_init_time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    TWUM71200043\n",
      "1    QMZSY1600015\n",
      "2    TWA530887303\n",
      "3    USSM11301446\n",
      "4    TWA471306001\n",
      "5    GBAYE1400079\n",
      "6    HKUM70704066\n",
      "7    USWD10423930\n",
      "8    HKA610200010\n",
      "9    TWI431000103\n",
      "Name: isrc, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print songs_extra['isrc'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ISRC is the standard recording code for songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isrc_to_year(isrc):\n",
    "    if type(isrc) == str:\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan\n",
    "        \n",
    "songs_extra['song_year'] = songs_extra['isrc'].apply(isrc_to_year)\n",
    "songs_extra.drop(['isrc', 'name'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge the members info into the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = train_file.merge(members, on='msno', how='left')\n",
    "test = test.merge(members, on='msno', how='left')\n",
    "\n",
    "train_file = train_file.merge(songs_extra, on = 'song_id', how = 'left')\n",
    "test = test.merge(songs_extra, on = 'song_id', how = 'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some GC since we're done with pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del members, songs; gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in train_file.columns:\n",
    "    if train_file[col].dtype == object:\n",
    "        train_file[col] = train_file[col].astype('category')\n",
    "        test[col] = test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_file.drop(['target'], axis=1)\n",
    "y = train_file['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = test.drop(['id'], axis=1)\n",
    "ids = test['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_file, test; gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(X, y)\n",
    "watchlist = [d_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n"
     ]
    }
   ],
   "source": [
    "#These parameters are purely arbitrarily chosen. The results may be better by choosing more params.\n",
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['learning_rate'] = 0.2\n",
    "params['application'] = 'binary'\n",
    "params['max_depth'] = 8\n",
    "params['num_leaves'] = 2**8\n",
    "params['verbosity'] = 0\n",
    "params['metric'] = 'auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\ttraining's auc: 0.720707\n",
      "[20]\ttraining's auc: 0.733158\n",
      "[30]\ttraining's auc: 0.740377\n",
      "[40]\ttraining's auc: 0.745327\n",
      "[50]\ttraining's auc: 0.749475\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(params, train_set=d_train, num_boost_round=50, valid_sets=watchlist, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions and saving them...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Making predictions and saving them...')\n",
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv('submission_lgbm.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
